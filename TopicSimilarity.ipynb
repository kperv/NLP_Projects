{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP exercise. Get bags-of-words, term frequencies and similarities of texts through word vectors\n",
    "# Corpus is four articles manually obtained: three small Daily Mail articles about the Duchess of Cambridge and her recent activities, \n",
    "# and one unrelated article on corolavirus deaths.\n",
    "# Expected to get high topic similarity on first three and low on the forth to others\n",
    "\n",
    "# The result down below is:\n",
    "\n",
    "# Same topic:\n",
    "# ------------------\n",
    "# Similarity of texts (1, 2) is 0.4896257193424013\n",
    "# Similarity of texts (1, 3) is 0.5677055366699043\n",
    "# Similarity of texts (2, 3) is 0.6053098409985186\n",
    "\n",
    "# Between topics\n",
    "# ------------------\n",
    "# Similarity of texts (1, 4) is 0.11484741046963563\n",
    "# Similarity of texts (2, 4) is 0.0969307223278006\n",
    "# Similarity of texts (3, 4) is 0.13043618289729517\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a text file with 4 articles. Every article stars with a # and a link to that article\n",
    "\n",
    "texts = list()\n",
    "i = 0\n",
    "\n",
    "with open('texts.txt', 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if line.startswith(\"#\") and not i:\n",
    "            string = '' \n",
    "            i += 1\n",
    "        elif line.startswith(\"#\") and i:\n",
    "            texts.append(string)\n",
    "            i += 1\n",
    "            string = ''\n",
    "        else:\n",
    "            string += line\n",
    "    texts.append(string)\n",
    "      \n",
    "assert len(texts) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# break texts into lists of tokens\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = list()\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    tokens.append(tokenizer.tokenize(text))\n",
    "    \n",
    "assert len(tokens) == len(texts)\n",
    "for i in range(len(texts)):\n",
    "    assert len(tokens[i]) != 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# clean up tokens from numbers or punctuation\n",
    "\n",
    "clean_tokens = list()\n",
    "\n",
    "for i, doci_tokens in enumerate(tokens):\n",
    "    clean_tokens.append(list())\n",
    "    for token in doci_tokens:\n",
    "        if token.isalpha():\n",
    "            clean_tokens[i].append(token.strip(\"'\").lower())\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "assert len(clean_tokens) == len(texts)\n",
    "for i in range(len(texts)):\n",
    "    assert len(clean_tokens[i]) != 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of stop words from tokens\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "clean_no_stop_tokens = list()\n",
    "\n",
    "for i, doci_text in enumerate(clean_tokens):\n",
    "    clean_no_stop_tokens.append([token for token in clean_tokens[i] if token not in stop_words])\n",
    "\n",
    "\n",
    "assert len(clean_tokens) != len(clean_tokens[i])\n",
    "for i in range(len(texts)):\n",
    "    assert len(clean_no_stop_tokens[i]) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['giovanna',\n",
       " 'fletcher',\n",
       " 'revealed',\n",
       " 'duchess',\n",
       " 'cambridge',\n",
       " 'easy',\n",
       " 'like',\n",
       " 'mum',\n",
       " 'met',\n",
       " 'motherhood',\n",
       " 'kate',\n",
       " 'middleton',\n",
       " 'appeared',\n",
       " 'special',\n",
       " 'episode',\n",
       " 'giovanna',\n",
       " 'happy',\n",
       " 'mum',\n",
       " 'happy',\n",
       " 'baby']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random eye-check\n",
    "\n",
    "clean_no_stop_tokens[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bags-of-words dicts\n",
    "\n",
    "text_counts = list()\n",
    "\n",
    "for i, doci_text in enumerate(clean_no_stop_tokens):\n",
    "    text_counts.append(dict(Counter(clean_no_stop_tokens[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('giovanna', 6), ('kate', 6), ('early', 6), ('said', 6), ('episode', 4), ('podcast', 4), ('years', 4), ('children', 4), ('time', 4), ('us', 4)]\n",
      "\n",
      "[('survey', 10), ('early', 8), ('james', 7), ('years', 7), ('duchess', 7), ('kate', 5), ('instagram', 5), ('big', 5), ('questions', 5), ('happy', 5)]\n",
      "\n",
      "[('podcast', 6), ('early', 6), ('kate', 5), ('years', 5), ('work', 5), ('said', 5), ('royal', 4), ('source', 4), ('duchess', 4), ('deeply', 3)]\n",
      "\n",
      "[('people', 14), ('said', 11), ('cases', 11), ('two', 9), ('italy', 9), ('health', 9), ('virus', 9), ('lombardy', 8), ('towns', 8), ('tested', 8)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the most popular terms in each \"bag\"\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    print(Counter(clean_no_stop_tokens[i]).most_common(10))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a corpus dictionary\n",
    "\n",
    "dictionary = list()\n",
    "\n",
    "for i, keys in enumerate(text_counts):\n",
    "    for key in text_counts[i]:\n",
    "        if key not in dictionary:\n",
    "            dictionary.append(key)\n",
    "\n",
    "dict_len = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dicts with term frequencies for each text by dividing term freqs to the corresponding lens of text_count dicts\n",
    "\n",
    "document_vector_lens = list()\n",
    "document_vectors = list()\n",
    "\n",
    "for i, values in enumerate(text_counts):\n",
    "    document_vector_lens.append(len(clean_no_stop_tokens[i]))\n",
    "    document_vectors.append(dict())\n",
    "    for (key, value) in text_counts[i].items():\n",
    "        assert document_vector_lens[i] != 0\n",
    "        document_vectors[i][key] = (value / document_vector_lens[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>able</th>\n",
       "      <th>abroad</th>\n",
       "      <th>according</th>\n",
       "      <th>across</th>\n",
       "      <th>activities</th>\n",
       "      <th>activity</th>\n",
       "      <th>actually</th>\n",
       "      <th>added</th>\n",
       "      <th>adhanom</th>\n",
       "      <th>...</th>\n",
       "      <th>works</th>\n",
       "      <th>world</th>\n",
       "      <th>worrying</th>\n",
       "      <th>would</th>\n",
       "      <th>written</th>\n",
       "      <th>wrote</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>youngest</th>\n",
       "      <th>zaia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>0.003067</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_3</th>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.020492</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.004539</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 766 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         abandon      able    abroad  according    across  activities  \\\n",
       "text_1  0.000000  0.003425  0.000000   0.000000  0.000000    0.000000   \n",
       "text_2  0.000000  0.000000  0.000000   0.000000  0.000000    0.000000   \n",
       "text_3  0.004098  0.000000  0.000000   0.000000  0.000000    0.000000   \n",
       "text_4  0.000000  0.000000  0.001513   0.004539  0.001513    0.003026   \n",
       "\n",
       "        activity  actually     added   adhanom  ...     works     world  \\\n",
       "text_1  0.000000  0.003425  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "text_2  0.000000  0.000000  0.003067  0.000000  ...  0.000000  0.000000   \n",
       "text_3  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "text_4  0.001513  0.000000  0.000000  0.001513  ...  0.001513  0.001513   \n",
       "\n",
       "        worrying     would   written     wrote      year     years  youngest  \\\n",
       "text_1  0.000000  0.003425  0.000000  0.000000  0.000000  0.013699  0.003425   \n",
       "text_2  0.000000  0.000000  0.003067  0.003067  0.000000  0.021472  0.003067   \n",
       "text_3  0.004098  0.000000  0.004098  0.000000  0.004098  0.020492  0.004098   \n",
       "text_4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "            zaia  \n",
       "text_1  0.000000  \n",
       "text_2  0.000000  \n",
       "text_3  0.000000  \n",
       "text_4  0.001513  \n",
       "\n",
       "[4 rows x 766 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a whole corpus table\n",
    "\n",
    "vectors = pd.DataFrame(document_vectors, index = ['text_1', 'text_2', 'text_3', 'text_4']).fillna(0)\n",
    "vectors.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating cosine similarity = v1 * v2 / |v1||v2|\n",
    "\n",
    "magnitudes = list()\n",
    "for i in range(len(texts)):\n",
    "    magnitudes.append(math.sqrt(vectors.iloc[i].map(lambda x: x*x).sum()))\n",
    "\n",
    "assert (magnitudes != 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of texts (1, 2) is 0.4896257193424013\n",
      "Similarity of texts (1, 3) is 0.5677055366699043\n",
      "Similarity of texts (1, 4) is 0.11484741046963563\n",
      "Similarity of texts (2, 3) is 0.6053098409985186\n",
      "Similarity of texts (2, 4) is 0.0969307223278006\n",
      "Similarity of texts (3, 4) is 0.13043618289729517\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "dot_products = list()\n",
    "similarities = list()\n",
    "\n",
    "comb = list()\n",
    "combinations = itertools.combinations((0,1,2,3), 2)\n",
    "for i,j in combinations:\n",
    "    comb.append((i,j))\n",
    "\n",
    "for i,j in comb:\n",
    "    dot_i_j = sum(np.multiply(vectors.values[i], vectors.values[j]))\n",
    "    dot_products.append(dot_i_j)\n",
    "    similarities.append(dot_i_j / (magnitudes[i] * magnitudes[j]))\n",
    "    \n",
    "for i in range(len(comb)):\n",
    "    print(\"Similarity of texts {} is {}\".format((comb[i][0] + 1, comb[i][1] + 1), similarities[i]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
