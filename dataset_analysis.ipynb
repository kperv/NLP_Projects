{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reviews.csv file and select two columns \n",
    "\n",
    "data_path = os.path.join(os.getcwd(), data, 'reviews.csv')\n",
    "            \n",
    "reviews = pd.read_csv(path)\n",
    "reviews = reviews[['country', 'description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "printing_precision = 20\n",
    "\n",
    "NGRAM_RANGE = (1, 3)\n",
    "TOP_K = 20000\n",
    "TOKEN_MODE = 'word'\n",
    "MIN_DOCUMENT_FREQUENCY = 5\n",
    "\n",
    "kwargs = {\n",
    "        'norm': None,\n",
    "        'stop_words': 'english',\n",
    "        'ngram_range': NGRAM_RANGE,\n",
    "        'dtype': 'int32',\n",
    "        'strip_accents': 'unicode',\n",
    "        'decode_error': 'replace',\n",
    "        'analyzer': TOKEN_MODE,\n",
    "        'min_df': MIN_DOCUMENT_FREQUENCY\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nclass_df(data_df, n_classes=3):\n",
    "    n_country_reviews = pd.DataFrame(columns=['country', 'description'])\n",
    "    top_countries = data_df.country.value_counts()[:n_classes].keys()\n",
    "    for country in top_countries:\n",
    "        country_reviews = pd.DataFrame(data_df[data_df.country == country])\n",
    "        n_country_reviews = n_country_reviews.append(country_reviews)\n",
    "    return n_country_reviews, top_countries\n",
    "\n",
    "def add_splits(data_df, classes, train_prop=0.7, val_prop=0.15, test_prop=0.15):\n",
    "    split_reviews = pd.DataFrame(columns=['country', 'description', 'split'])\n",
    "    for country in classes:\n",
    "        country_reviews = pd.DataFrame(data_df[data_df.country == country])\n",
    "        n_total = len(country_reviews)\n",
    "        n_train = int(n_total * train_prop)\n",
    "        n_val = int(n_total * val_prop)\n",
    "        n_test = int(n_total * test_prop)\n",
    "\n",
    "        country_reviews['split'] = None\n",
    "        country_reviews.split.iloc[:n_train] = 'train'\n",
    "        country_reviews.split.iloc[n_train:n_train+n_val] = 'val'\n",
    "        country_reviews.split.iloc[n_train+n_val:] = 'test'\n",
    "\n",
    "        split_reviews = split_reviews.append(country_reviews)\n",
    "    return split_reviews\n",
    "\n",
    "\n",
    "reviews, classes = get_nclass_df(reviews, n_classes)\n",
    "reviews = add_splits(reviews, classes)\n",
    "train_corpus = reviews[reviews.split == 'train'].description.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorized_texts = vectorizer.fit_transform(train_corpus)\n",
    "print(\"vocabulary size: {}\".format(len(vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ngrams = list(vectorizer.get_feature_names())\n",
    "num_ngrams = len(all_ngrams)\n",
    "\n",
    "all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
    "result = [(c, n) for c, n in sorted(zip(all_counts, all_ngrams), reverse=True)]\n",
    "all_counts, all_ngrams = zip(*result)\n",
    "\n",
    "ngrams = list(all_ngrams)[:printing_precision]\n",
    "counts = list(all_counts)[:prining_precision]\n",
    "\n",
    "idx = np.arange(prining_precision)\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.bar(idx, counts, width=.8)\n",
    "plt.xlabel('N_grams')\n",
    "plt.ylabel('Frequencies')\n",
    "plt.title('Frequency distribution of ngrams')\n",
    "plt.xticks(idx, ngrams, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = reviews.country.value_counts().values\n",
    "classes = reviews.country.unique()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(classes, counts, width=.8, color='r')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title('Class distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lens = reviews.description.str.len().values\n",
    "\n",
    "plt.hist(review_lens, printing_precision)\n",
    "plt.xlabel('Length of a sample')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title('Sample length distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(**kwargs)\n",
    "x_train = vectorizer.fit_transform(train_corpus).astype('float32')\n",
    "y_train = (reviews[reviews.split == 'train'].country == 'US').astype(int).values\n",
    "\n",
    "max_value = x_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print(\"Feature names with lowest tfidf:\\n{}\".format(\n",
    "feature_names[sorted_by_tfidf[:printing_precision]]))\n",
    "\n",
    "print(\"Feature names with highest tfidf:\\n{}\".format(\n",
    "feature_names[sorted_by_tfidf[-printing_precision:]]))\n",
    "\n",
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "feature_names[sorted_by_idf[:printing_precision]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(x_train, y_train)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "\n",
    "clf = grid.best_estimator_.fit(x_train, y_train)\n",
    "coef = clf.coef_.ravel()\n",
    "list = [(n, c) for n, c in zip(feature_names, coef)]\n",
    "list = sorted(list, key=lambda x: x[1])\n",
    "list = list[:20] + list[-20:]\n",
    "\n",
    "words, coef = zip(*list)\n",
    "idx = np.arange(len(words))\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.bar(idx, coef, width=0.8)\n",
    "plt.xticks(idx, words, rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 2\n",
    "\n",
    "\n",
    "clf = grid.best_estimator_.fit(x_train, y_train)\n",
    "coef = clf.coef_.ravel()[mask]\n",
    "list = [(n, c) for n, c in zip(feature_names[mask], coef)]\n",
    "list = sorted(list, key=lambda x: x[1])\n",
    "list = list[:20] + list[-20:]\n",
    "\n",
    "words, coef = zip(*list)\n",
    "idx = np.arange(len(words))\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.bar(idx, coef, width=0.8)\n",
    "plt.xticks(idx, words, rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n",
    "\n",
    "\n",
    "clf = grid.best_estimator_.fit(x_train, y_train)\n",
    "coef = clf.coef_.ravel()[mask]\n",
    "list = [(n, c) for n, c in zip(feature_names[mask], coef)]\n",
    "list = sorted(list, key=lambda x: x[1])\n",
    "list = list[:20] + list[-20:]\n",
    "\n",
    "words, coef = zip(*list)\n",
    "idx = np.arange(len(words))\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.bar(idx, coef, width=0.8)\n",
    "plt.xticks(idx, words, rotation=60)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
