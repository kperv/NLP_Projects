{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "98000/98000 [==============================] - 6s 64us/step - loss: 1.7119 - acc: 0.6389\n",
      "Epoch 2/5\n",
      "98000/98000 [==============================] - 6s 58us/step - loss: 1.5369 - acc: 0.6406\n",
      "Epoch 3/5\n",
      "98000/98000 [==============================] - 6s 59us/step - loss: 1.5356 - acc: 0.6406\n",
      "Epoch 4/5\n",
      "98000/98000 [==============================] - 6s 59us/step - loss: 1.5344 - acc: 0.6406\n",
      "Epoch 5/5\n",
      "98000/98000 [==============================] - 6s 59us/step - loss: 1.5334 - acc: 0.6406\n",
      "2000/2000 [==============================] - 0s 166us/step\n",
      "Model metrics names: ['loss', 'acc']\n",
      "Test score: [1.5492809972763062, 0.6275]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "An attempt to analyse food reviews with Neural Network.\n",
    "The simplest 3 layer network. \n",
    "Result was of no great interest, but the working toy NN implementation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "\"\"\"\n",
    "the reviews is a rather big file, \n",
    "so it might be convenient to change the size\n",
    "according to the immediate task, such as fast testing of NN parameters\n",
    "\"\"\"\n",
    "DATA_SET_SIZE = 100000\n",
    "TEST_SIZE = 2000\n",
    "WORD_INDEX_SIZE = 10000\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Get the data from the file on computer (need to be downloaded), \n",
    "    take relevant columns and rows according to the DATA_SET_SIZE.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.read_csv('Reviews.csv')\n",
    "    data = data.loc[:, ['Score', 'Text']]\n",
    "    data = shuffle(data)\n",
    "    data = data.iloc[:DATA_SET_SIZE, :]\n",
    "    word_index = get_word_index(WORD_INDEX_SIZE)\n",
    "    \n",
    "    test = data.iloc[:TEST_SIZE, :]\n",
    "    data = data.iloc[TEST_SIZE:, :]\n",
    "\n",
    "    test_data = test['Text']\n",
    "    test_data_seq = get_sequences(test_data)\n",
    "    X_test = encode_review(test_data_seq)\n",
    "\n",
    "    test_labels = test['Score']\n",
    "    test_labels = get_labels(test_labels)\n",
    "    one_hot_test_labels = to_one_hot(test_labels)\n",
    "\n",
    "    train_data = data['Text']\n",
    "    train_data_seq = get_sequences(train_data)\n",
    "    X_train = encode_review(train_data_seq)\n",
    "\n",
    "    train_labels = data['Score']\n",
    "    train_labels = get_labels(train_labels)\n",
    "    one_hot_train_labels = to_one_hot(train_labels)\n",
    "\n",
    "    model = get_model()\n",
    "    test_score = model.evaluate(X_test, one_hot_test_labels)\n",
    "    print(\"Model metrics names: {}\".format(model.metrics_names))\n",
    "    print(\"Test score: {}\".format(test_score))\n",
    "\n",
    "def clean_review(review):\n",
    "    cl_review = []\n",
    "    review = re.sub(\"<.*?>\", \" \", review)\n",
    "    review = ''.join([i for i in review if not i.isdigit()])\n",
    "    review = review.split()\n",
    "    for word in review:\n",
    "        word = word.lower()\n",
    "        word = word.strip(string.punctuation)\n",
    "        if len(word) > 0:\n",
    "            cl_review.append(word)\n",
    "    return cl_review\n",
    "\n",
    "def get_word_index(num_most_common=1000):\n",
    "    \n",
    "    list_of_words = []\n",
    "    for i, review in enumerate(data['Text']):\n",
    "        review = clean_review(review)\n",
    "        for word in review:\n",
    "            list_of_words.append(word)\n",
    "\n",
    "    word_fq = Counter(list_of_words).most_common(num_most_common)\n",
    "\n",
    "    word_index = {}\n",
    "    for i, (w, c) in enumerate(word_fq):\n",
    "        word_index[w] = i\n",
    "    return word_index\n",
    "    \n",
    "def get_sequences(data):\n",
    "    sequences = []\n",
    "    for i, rev in data.iteritems():\n",
    "        rev = clean_review(rev)\n",
    "        review = []\n",
    "        for word in rev:\n",
    "            try:\n",
    "                word_index[word]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            else:\n",
    "                review.append(word_index[word])\n",
    "        sequences.append(review)\n",
    "    return np.array(sequences)\n",
    "    \n",
    "def encode_review(sequences, dimension=WORD_INDEX_SIZE):\n",
    "    result = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        result[i, sequence] = 1.\n",
    "    return result\n",
    "\n",
    "def get_labels(data):\n",
    "    labels = []\n",
    "    for i, label in data.iteritems():\n",
    "        labels.append(label-1)\n",
    "    return np.array(labels)\n",
    "\n",
    "def to_one_hot(labels, dimension=5):\n",
    "    result = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        result[i, label] = 1.\n",
    "    return result\n",
    "\n",
    "def get_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(8, kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001), \n",
    "                           activation='relu', input_shape=(WORD_INDEX_SIZE, )))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(64, kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.001), \n",
    "                           activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(5, activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "    model.fit(X_train, one_hot_train_labels, epochs=5, batch_size=125, verbose=1)\n",
    "    return model\n",
    "\n",
    "main()\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
